{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "# analyze_cooccurrency_themepark.py\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# 使用する単語の品詞とストップワードの指定\n",
    "include_pos = ('NOUN', 'VERB', 'ADJ', 'PROPN')\n",
    "stopwords = ('する', 'ある', 'ない', 'いう', 'もの', 'こと', 'よう',\n",
    "             'なる', 'ほう', 'いる', 'くる', 'お', 'つ', 'おる', 'とき', 'しまう',\n",
    "             'いく', 'みる', 'やる', 'ため', 'ところ', '際', '他', '時', '中', '方', '回', '目',\n",
    "             '年', '月', '日', '分', '事', '等', '内', '間', '半', '頃', '前', '後', '以上',\n",
    "             '思う', '行う', '行く', '行なう', '行ける', '来る', '入る', '思う', '見る', '見える', '見れる', '観る', '訪れる')\n",
    "\n",
    "\n",
    "# 東京ディズニーランドの口コミのファイルの読み込み\n",
    "df = pd.read_csv('text/tds.csv', encoding='utf-8',\n",
    "                 parse_dates=['旅行の時期'])\n",
    "\n",
    "# 空行を削除\n",
    "df['口コミ'] = df['口コミ'].replace( '\\n+', '\\n', regex=True)\n",
    "\n",
    "# 旅行の時期が欠損している口コミを削除する\n",
    "df = df.dropna(subset=['旅行の時期'])\n",
    "\n",
    "# 旅行の時期を年度に直し，「年度」という項目を追加\n",
    "df['年度'] = df['旅行の時期'].apply(lambda x: x.year if x.month >= 4 else x.year-1)\n",
    "\n",
    "# 旅行の時期を指定　ここでは2016年度から2018年度とする\n",
    "from_year = 2014\n",
    "to_year = 2018\n",
    "df = df[(df['年度'] >= from_year) & (df['年度'] <= to_year)]\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    words = [token.lemma_ for token in sent\n",
    "             if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def count_cooccurrence(sents, token_length='{2,}'):\n",
    "    token_pattern=f'\\\\b\\\\w{token_length}\\\\b'\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "\n",
    "    X = count_model.fit_transform(sents)\n",
    "    #words = count_model.get_feature_names_out()    こっちだと動かない\n",
    "    words = count_model.get_feature_names()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1 # limit to 1 occurrence in a document.\n",
    "    Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_weights(words, word_counts):\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {'weight': count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights\n",
    "\n",
    "def cooccurrence_weights(words, Xc, weight_cutoff):\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = weight_cutoff * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i,j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i,j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "    \n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    \n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G\n",
    "\n",
    "def pyplot_network(G):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "        \n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def nx2pyvis_G(G):\n",
    "    pyvis_G = Network(width='800px', height='800px', notebook=True)\n",
    "    # pyvis_G.from_nx(G) # pyvisライブラリ現状では，属性が反映されない．\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs['weight'])\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs['weight'])\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "# visualize-2から,引数としてweight_cutoffを渡すように変更\n",
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "    \n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    \n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G\n",
    "\n",
    "def pyplot_network(G):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "        \n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def nx2pyvis_G(G):\n",
    "    pyvis_G = Network(width='800px', height='800px', notebook=True)\n",
    "    # pyvis_G.from_nx(G) # pyvisライブラリ現状では，属性が反映されない．\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs['weight'],font={\"size\":20 })\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs['weight'])\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize-2から文のリストへの追加方法を変更\n",
    "sents = []\n",
    "for doc in nlp.pipe(df['口コミ']):\n",
    "    sents.extend([' '.join(extract_words(sent, include_pos, stopwords))\n",
    "                  for sent in doc.sents])\n",
    "    \n",
    "words, word_counts, Xc, X = count_cooccurrence(sents, '{1,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cutoff = 0.020\n",
    "G = create_network(words, word_counts, Xc, weight_cutoff)\n",
    "pyplot_network(G)\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show(\"mygraph_tds.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
