{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18aa6dbf-9b62-46ad-973f-7e219f041738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7241b-b6ff-4b3e-9976-679a6ea8f4d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 共起ネットワークの表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb9aac-2c31-4db6-adca-7256c47cb6fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "このセルの実行には，「ginzaの使い方」ノートブックにある，「パッケージのインストール」と「解析対象のデータをダウンロード」を予め実行しておく必要がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff28ba-17cb-4d48-a9ae-98c82274608f",
   "metadata": {
    "tags": []
   },
   "source": [
    "まずは，「ginzaの使い方」ノートブックの「単語の共起」と同じ処理を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb986e32-15d5-490b-a56e-89c45a3b238e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "input_fn = 'text/kageotoko.corpus.txt'\n",
    "\n",
    "include_pos = ('NOUN', 'VERB', 'ADJ')\n",
    "stopwords = ('する', 'ある', 'ない', 'いう', 'もの', 'こと', 'よう', 'なる', 'ほう', 'いる', 'くる')\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0a4a2-4f45-452b-a4a8-7cf229cc39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    words = [token.lemma_ for token in sent\n",
    "             if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95509e55-2b92-4aa2-b72a-034dd48ae5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def count_cooccurrence(sents, token_length='{2,}'):\n",
    "    token_pattern=f'\\\\b\\\\w{token_length}\\\\b'\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "\n",
    "    X = count_model.fit_transform(sents)\n",
    "    words = count_model.get_feature_names_out()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1 # limit to 1 occurrence in a document.\n",
    "    Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d8136-4050-4f5c-a6a8-9e157a54494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_fn) as f:\n",
    "    text = f.read()\n",
    "\n",
    "doc = nlp(text)\n",
    "sents = [' '.join(extract_words(sent, include_pos, stopwords))\n",
    "          for sent in doc.sents]\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2de3ba-5759-4644-aa26-782bb549ed22",
   "metadata": {},
   "source": [
    "共起ネットワーク図を見やすくするために，単語の出現数に応じた頂点の大きさ，共起の多さに応じた辺の太さを表示し分ける．そのための，重みを求める関数を作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5426c-9a81-4fa4-b892-3dcd7337a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_weights(words, word_counts):\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {'weight': count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights\n",
    "\n",
    "def cooccurrence_weights(words, Xc, weight_cutoff):\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = weight_cutoff * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i,j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i,j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf173979-172a-4b33-81c7-c42c74a2ed6b",
   "metadata": {},
   "source": [
    "重み付き頂点と重み付き辺からグラフを作成する関数を作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed225f0c-e394-4554-82f3-d9042f833f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "    \n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    \n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G\n",
    "\n",
    "def pyplot_network(G):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "        \n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def nx2pyvis_G(G):\n",
    "    pyvis_G = Network(width='800px', height='800px', notebook=True)\n",
    "    # pyvis_G.from_nx(G) # pyvisライブラリ現状では，属性が反映されない．\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs['weight'])\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs['weight'])\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ce742-e2d6-40ab-bd42-2774496a1eb5",
   "metadata": {},
   "source": [
    "以上の関数を呼び出して，グラフを描画する．２つの共起ネットワークが表示され，最初の図は静的なもの，次の図はブラウザでの閲覧によってマウス操作でネットワークのノードの配置を動かすことができる．ノードが密集している部分は，着目したノードをマウスで掴んで引っ張って，そのノードに結合している他のノードを視覚的に捉えることができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d28b0c-cafb-4456-ad22-5c5865854317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = create_network(words, word_counts, Xc, 0.03)\n",
    "pyplot_network(G)\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show(\"mygraph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62519-efd3-46c2-8e9a-ee97f6761171",
   "metadata": {},
   "source": [
    "共起ネットワークを見ているうちに，「人間」と「探求」の共起に興味が及んだので，原文を見てみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceeb982-1301-447e-8e73-1fded1006229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_by_cooccurrence(X, idxs):\n",
    "    occur_flags = (X[:,idxs[0]] > 0)\n",
    "    for idx in idxs[1:]:\n",
    "        occur_flags = occur_flags.multiply(X[:,idx] > 0)\n",
    "    return occur_flags.nonzero()[0]\n",
    "\n",
    "sents_orig = list(doc.sents)\n",
    "words_lookup = {word: index for index, word in enumerate(words)}\n",
    "idxs = [words_lookup[word] for word in ['人間', '探求']]\n",
    "\n",
    "for i in find_sentence_by_cooccurrence(X, idxs):\n",
    "    print(\"{:>5d}: {}\".format(i, sents_orig[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
