{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b00052",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chapter04から使用するテキストデータをコピーする（事前にChapter04/Sentiment.ipynbを実行してください）\n",
    "!cp -r ../Chapter04/text ./\n",
    "!cp -r ../Chapter04/sisyou_db ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    words = [token.lemma_ for token in sent\n",
    "             if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def count_cooccurrence(tokens):    \n",
    "    count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "    X = count_model.fit_transform(tokens)\n",
    "    words = count_model.get_feature_names()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1 # limit to 1 occurrence in a document.\n",
    "    Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_weights(words, word_counts):\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {'weight': count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights\n",
    "\n",
    "def cooccurrence_weights(words, Xc):\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = 0.01 * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i,j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i,j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_network(words, word_counts, Xc):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "    \n",
    "    weights_c = cooccurrence_weights(words, Xc)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    \n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G\n",
    "\n",
    "def pyplot_network(G):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "        \n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def nx2pyvis_G(G):\n",
    "    pyvis_G = Network(width='800px', height='800px', notebook=True)\n",
    "    # pyvis_G.from_nx(G) # pyvisライブラリ現状では，属性が反映されない．\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs['weight'])\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs['weight'])\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = \"text/kageotoko.corpus.txt\"\n",
    "with open(input_fn) as f:\n",
    "    text = f.read()\n",
    "\n",
    "include_pos = ('PROPN')\n",
    "stopwords = ()\n",
    "\n",
    "doc = nlp(text)\n",
    "sents = [' '.join(extract_words(sent, include_pos, stopwords))\n",
    "          for sent in doc.sents]\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_network(words, word_counts, Xc)\n",
    "pyplot_network(G)\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show(\"mygraph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [[{\"POS\": \"NOUN\"}] * n for n in [2,3,4]]\n",
    "patterns.extend([[{\"POS\": \"PROPN\"}] * n for n in [2,3,4]])\n",
    "patterns.append([{\"POS\": \"NOUN\"},{\"POS\": \"PROPN\"}])\n",
    "patterns.append([{\"POS\": \"NOUN\"},{\"POS\": \"PROPN\"},{\"POS\": \"PROPN\"}])\n",
    "patterns.append([{\"POS\": \"PROPN\"},{\"POS\": \"NOUN\"}])\n",
    "patterns.append([{\"POS\": \"PROPN\"},{\"POS\": \"NOUN\"},{\"POS\": \"NOUN\"}])\n",
    "## GiNZA 5.0.X\n",
    "#for pattern in patterns:\n",
    "#    name = f'noun_phrase_{len(pattern)}'\n",
    "#    matcher.add(name, [pattern])\n",
    "\n",
    "# GiNZA 4.0.X\n",
    "for pattern in patterns:\n",
    "    name = f'propn_{len(pattern)}'\n",
    "    matcher.add(name, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = \"text/kageotoko.corpus.txt\"\n",
    "\n",
    "with open(input_fn) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for doc in nlp.pipe([text]):\n",
    "    nps = [doc[begin:end].text for _, begin, end in matcher(doc)]\n",
    "    counter.update(nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"characters_raw.txt\",\"w\") as fout:\n",
    "    for word, count in counter.most_common(200):\n",
    "        fout.write(\"{}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辞書のシーズとなる人名リストを作成\n",
    "* cp characters_raw.txt characters.txt\n",
    "* characters.txt を目視でチェックして、人名のみを残す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sudachipy用の辞書ファイルを作成\n",
    "\n",
    "file_character = \"characters.txt\"\n",
    "file_dic = \"dic_characters.txt\"\n",
    "\n",
    "\n",
    "with open(file_dic,\"w\") as fout:\n",
    "     for word in open(file_character):\n",
    "          word = word.rstrip()\n",
    "          fout.write(\"{},4789,4789,5000,{},名詞,固有名詞,一般,*,*,*,*,{},*,*,*,*,*\\n\".format(word,word,word))\n",
    "          print(\"{},4789,4789,5000,{},名詞,固有名詞,一般,*,*,*,*,{},*,*,*,*,*\".format(word,word,word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ユーザ辞書(user.dic)の生成\n",
    "!sudachipy ubuild -s ~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/sudachidict_core/resources/system.dic  dic_characters.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sudachi.jsonを編集\n",
    "!vi  ~/.pyenv/versions/3.9.5/lib/python3.9/site-packages/sudachipy/resources/sudachi.json \n",
    "```\n",
    "\"characterDefinitionFile\" : \"char.def\",\n",
    "\"userDict\" : [\"/content/user.dic\"],      ← ユーザー辞書ファイルのパスを指定\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"明智小五郎っていう私立探偵知ってるでしょう？\"\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token,token.pos_,token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = \"text/kageotoko.corpus.txt\"\n",
    "with open(input_fn) as f:\n",
    "    text = f.read()\n",
    "\n",
    "include_pos = ('PROPN')\n",
    "stopwords = ()\n",
    "\n",
    "doc = nlp(text)\n",
    "sents = [' '.join(extract_words(sent, include_pos, stopwords))\n",
    "          for sent in doc.sents]\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_network(words, word_counts, Xc)\n",
    "pyplot_network(G)\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show(\"mygraph.html\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18018e41722abe2d0132f8df803b6bc8ce2c6c557cb152ba443126053cce7fe4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ginza4.0': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
